# -*- coding: utf-8 -*-
"""baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19cwwlTxkLMOXjfQ3lj2uB7kDttwCheGg

# –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π UNet –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏

–í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ UNet –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ CamVid. –ò—Å–ø–æ–ª—å–∑—É–µ–º PyTorch Lightning –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É segmentation_models_pytorch –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏.
"""

!pip install -q segmentation-models-pytorch albumentations
!pip install pytorch_lightning

import os
import shutil
import zipfile
import requests
from tqdm import tqdm

def download_full_camvid():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç CamVid —Å kaggle"""
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ CamVid...")

    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
    if os.path.exists('/content/CamVid'):
        shutil.rmtree('/content/CamVid')

    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
    temp_dir = '/content/temp_camvid'
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    os.makedirs(temp_dir, exist_ok=True)

    url = "https://www.kaggle.com/api/v1/datasets/download/carlolepelaars/camvid"
    zip_path = os.path.join(temp_dir, "master.zip")

    print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ...")
    response = requests.get(url, stream=True)
    total_size = int(response.headers.get('content-length', 0))

    with open(zip_path, 'wb') as f, tqdm(
        desc="–ü—Ä–æ–≥—Ä–µ—Å—Å",
        total=total_size,
        unit='B',
        unit_scale=True,
        unit_divisor=1024,
    ) as pbar:
        for data in response.iter_content(chunk_size=1024):
            f.write(data)
            pbar.update(len(data))

    # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º
    print("–†–∞—Å–ø–∞–∫–æ–≤–∫–∞...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)

    # –û—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
    source_dir = os.path.join(temp_dir, 'CamVid')

    # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    target_dir = '/content/CamVid'
    os.makedirs(target_dir, exist_ok=True)

    # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö –∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–∞–ø–æ–∫
    # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: train -> train, val -> val, test -> test
    # –ú–∞—Å–∫–∏: train_labels -> trainannot, val_labels -> valannot, test_labels -> testannot
    split_mapping = {
        'train': ('train', 'train_labels'),
        'val': ('val', 'val_labels'),
        'test': ('test', 'test_labels')
    }

    for target_split, (src_images, src_masks) in split_mapping.items():
        # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        src_img_dir = os.path.join(source_dir, src_images)
        dst_img_dir = os.path.join(target_dir, target_split)

        if os.path.exists(src_img_dir):
            shutil.copytree(src_img_dir, dst_img_dir, dirs_exist_ok=True)
            print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {target_split}: {len(os.listdir(src_img_dir))} —Ñ–∞–π–ª–æ–≤")
        else:
            print(f"–ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {src_img_dir}")

        # –ö–æ–ø–∏—Ä—É–µ–º –º–∞—Å–∫–∏ (–ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º *_labels –≤ *annot)
        src_mask_dir = os.path.join(source_dir, src_masks)
        dst_mask_dir = os.path.join(target_dir, f"{target_split}annot")

        if os.path.exists(src_mask_dir):
            shutil.copytree(src_mask_dir, dst_mask_dir, dirs_exist_ok=True)
            print(f"–ú–∞—Å–∫–∏ {target_split}: {len(os.listdir(src_mask_dir))} —Ñ–∞–π–ª–æ–≤")
        else:
            print(f"–ü–∞–ø–∫–∞ —Å –º–∞—Å–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {src_mask_dir}")

    # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª class_dict.csv
    src_class_file = os.path.join(source_dir, 'class_dict.csv')
    dst_class_file = os.path.join(target_dir, 'class_dict.csv')
    if os.path.exists(src_class_file):
        shutil.copy(src_class_file, dst_class_file)
        print("class_dict.csv —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω")

    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
    shutil.rmtree(temp_dir)

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    print("\n" + "="*50)
    print("–ò–¢–û–ì–û–í–ê–Ø –ü–†–û–í–ï–†–ö–ê –°–¢–†–£–ö–¢–£–†–´:")
    print("="*50)

    for split in ['train', 'val', 'test']:
        img_dir = os.path.join(target_dir, split)
        mask_dir = os.path.join(target_dir, f"{split}annot")

        if os.path.exists(img_dir):
            img_count = len([f for f in os.listdir(img_dir) if f.endswith('.png')])
            print(f"{split}: {img_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        else:
            print(f"{split}: –ø–∞–ø–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        if os.path.exists(mask_dir):
            mask_count = len([f for f in os.listdir(mask_dir) if f.endswith('.png')])
            print(f"{split}annot: {mask_count} –º–∞—Å–æ–∫")
        else:
            print(f"{split}annot: –ø–∞–ø–∫–∞ –º–∞—Å–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

    print("\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞!")

# –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É
download_full_camvid()

def create_full_txt_files():
    """–°–æ–∑–¥–∞–µ—Ç .txt —Ñ–∞–π–ª—ã –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    data_dir = '/content/CamVid'

    for split in ['train', 'val', 'test']:
        txt_path = os.path.join(data_dir, f"{split}.txt")
        img_dir = os.path.join(data_dir, split)

        img_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])

        with open(txt_path, 'w') as f:
            for img_file in img_files:
                img_abs = os.path.abspath(os.path.join(img_dir, img_file))
                mask_abs = os.path.abspath(os.path.join(data_dir, f"{split}annot", img_file.replace('.png', '_L.png')))

                # –í –∏—Å—Ö–æ–¥–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –º–∞—Å–∫–∏ –∏–º–µ—é—Ç —Å—É—Ñ—Ñ–∏–∫—Å _L
                if not mask_abs.endswith('_L.png'):
                    mask_abs = mask_abs.replace('.png', '_L.png')

                if os.path.exists(mask_abs):
                    f.write(f"{img_abs} {mask_abs}\n")
                else:
                    # –ü–æ–ø—Ä–æ–±—É–µ–º –±–µ–∑ _L —Å—É—Ñ—Ñ–∏–∫—Å–∞
                    mask_abs_no_L = mask_abs.replace('_L.png', '.png')
                    if os.path.exists(mask_abs_no_L):
                        f.write(f"{img_abs} {mask_abs_no_L}\n")

        print(f"–°–æ–∑–¥–∞–Ω {split}.txt —Å {len(img_files)} –∑–∞–ø–∏—Å—è–º–∏")

create_full_txt_files()

!find /content/CamVid -name "*.png" | wc -l

# ============ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ============
download_full_camvid()
create_full_txt_files()

# ============ –ü–†–û–í–ï–†–ö–ê –°–û–ó–î–ê–ù–ò–Ø –§–ê–ô–õ–û–í ============
print("\nüîç –ü–†–û–í–ï–†–ö–ê –§–ê–ô–õ–û–í –ü–û–°–õ–ï –ó–ê–ì–†–£–ó–ö–ò:")
data_dir = '/content/CamVid'
if not os.path.exists(data_dir):
    print(f"‚ùå –ü–∞–ø–∫–∞ {data_dir} –ù–ï —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
else:
    print(f"‚úÖ –ü–∞–ø–∫–∞ {data_dir} —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

    for split in ['train', 'val', 'test']:
        txt_path = os.path.join(data_dir, f"{split}.txt")
        if os.path.exists(txt_path):
            with open(txt_path, 'r') as f:
                lines = f.readlines()
            print(f"‚úÖ {split}.txt: {len(lines)} –∑–∞–ø–∏—Å–µ–π")
            if lines:
                print(f"   –ü–µ—Ä–≤–∞—è –∑–∞–ø–∏—Å—å: {lines[0].strip()}")
        else:
            print(f"‚ùå {split}.txt –ù–ï –ù–ê–ô–î–ï–ù!")

        # –ü—Ä–æ–≤–µ—Ä–∏–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–æ–∫ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –º–∞—Å–∫–∞–º–∏
        img_dir = os.path.join(data_dir, split)
        mask_dir = os.path.join(data_dir, f"{split}annot")
        if os.path.exists(img_dir):
            imgs = [f for f in os.listdir(img_dir) if f.endswith('.png')]
            print(f"   {split}: {len(imgs)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        else:
            print(f"   {split}: –ø–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

        if os.path.exists(mask_dir):
            masks = [f for f in os.listdir(mask_dir) if f.endswith('.png')]
            print(f"   {split}annot: {len(masks)} –º–∞—Å–æ–∫")
        else:
            print(f"   {split}annot: –ø–∞–ø–∫–∞ —Å –º–∞—Å–∫–∞–º–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

"""## –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º UNet, PyTorch Lightning –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger
import numpy as np
from PIL import Image
import os
from tqdm import tqdm
import matplotlib.pyplot as plt
import segmentation_models_pytorch as smp
import albumentations as A
from albumentations.pytorch import ToTensorV2
import warnings
warnings.filterwarnings('ignore')

# –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
torch.manual_seed(42)
np.random.seed(42)
pl.seed_everything(42)

"""## –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö

–û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ albumentations.

"""

''' def get_train_augmentation(img_size=(256, 256)):
    return A.Compose([
        A.Resize(img_size[0], img_size[1]),
        A.HorizontalFlip(p=0.3),
        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=5, p=0.3),
        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

def get_val_augmentation(img_size=(256, 256)):
    return A.Compose([
        A.Resize(img_size[0], img_size[1]),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
'''

def get_train_augmentation_256():
    """–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –Ω–µ–±–æ"""
    return A.Compose([
        A.Resize(256, 256),
        A.HorizontalFlip(p=0.5),
        A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=0.7),
        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),
        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=10, p=0.5),
        #A.RandomCrop(height=230, width=230, p=0.3),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

def get_val_augmentation_256():
    """–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    return A.Compose([
        A.Resize(256, 256),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, alpha=None, ignore_index=11):
        super().__init__()
        self.gamma = gamma
        self.ignore_index = ignore_index

        # –í–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ CamVid (—ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ)
        if alpha is None:
            self.alpha = torch.tensor([
                0.3,   # Sky (0)
                1.2,   # Building (1)
                2.5,   # Column-Pole (2)
                0.5,   # Road (3)
                1.5,   # Sidewalk (4)
                1.0,   # Tree (5)
                3.0,   # Sign-Symbol (6)
                2.0,   # Fence (7)
                1.0,   # Car (8)
                4.0,   # Pedestrian (9)
                4.0,   # Bicyclist (10)
                0.0    # Void (11) - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
            ])
        else:
            self.alpha = alpha

        self.register_buffer('alpha_buffer', self.alpha)

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(
            inputs,
            targets,
            reduction='none',
            ignore_index=self.ignore_index
        )

        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma) * ce_loss

        if self.alpha.device != inputs.device:
            self.alpha = self.alpha.to(inputs.device)

        # –í–∑–≤–µ—à–∏–≤–∞–µ–º –ø–æ –∫–ª–∞—Å—Å–∞–º
        alpha_weight = self.alpha_buffer[targets]
        focal_loss = focal_loss * alpha_weight

        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–∏–∫—Å–µ–ª–∏ —Å ignore_index
        mask = (targets != self.ignore_index).float()
        return (focal_loss * mask).sum() / mask.sum()

"""## –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏

–ò—Å–ø–æ–ª—å–∑—É–µ–º CamVid –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç 12 –∫–ª–∞—Å—Å–æ–≤ –¥–æ—Ä–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω.

"""

class CamVidDataset(Dataset):
    def __init__(self, data_dir, split='train', img_size=(256, 256), transform=None):
        self.data_dir = data_dir
        self.img_size = img_size
        self.transform = transform

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—É—Ç–µ–π –∏–∑ .txt —Ñ–∞–π–ª–∞
        txt_file = os.path.join(data_dir, split + '.txt')
        self.samples = []

        with open(txt_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line:
                    parts = line.split()
                    if len(parts) >= 2:
                        img_path_abs = parts[0]
                        mask_path_abs = parts[1]

                        img_filename = os.path.basename(img_path_abs)
                        mask_filename = os.path.basename(mask_path_abs)

                        img_path = os.path.join(data_dir, split, img_filename)
                        mask_path = os.path.join(data_dir, split + 'annot', mask_filename)

                        if os.path.exists(img_path) and os.path.exists(mask_path):
                            self.samples.append((img_path, mask_path))

        print(f"Loaded {len(self.samples)} samples from {split} split")

    @staticmethod
    def normalize_camvid_mask(mask_array):
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–∞—Å–∫—É CamVid –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º –∫–ª–∞—Å—Å–æ–≤ 0-11
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ 128 (Void –∫–ª–∞—Å—Å ‚Üí 11)
        """
        normalized = mask_array.copy()

        if 128 in mask_array:
            normalized[mask_array == 128] = 11

        mask_other = (mask_array != 128) & (mask_array <= 127)
        if mask_other.any():
            # –ü—Ä–æ—Å—Ç–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: 0-127 ‚Üí 0-10
            normalized[mask_other] = mask_array[mask_other] // 12

        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω
        normalized = np.clip(normalized, 0, 11)

        return normalized.astype(np.uint8)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, mask_path = self.samples[idx]

        # –ó–∞–≥—Ä—É–∑–∫–∞
        image = np.array(Image.open(img_path).convert('RGB'))
        mask = np.array(Image.open(mask_path).convert('L'))


        mask = self.normalize_camvid_mask(mask)

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
        if self.transform:
            transformed = self.transform(image=image, mask=mask)
            image = transformed['image']
            mask = transformed['mask']

            if isinstance(image, np.ndarray):
                image = torch.from_numpy(image).permute(2, 0, 1).float()
            if isinstance(mask, np.ndarray):
                mask = torch.from_numpy(mask).long()
        else:
            # –ë–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
            image = Image.fromarray(image).resize(self.img_size, Image.BILINEAR)
            image = np.array(image).astype(np.float32) / 255.0
            mask = Image.fromarray(mask).resize(self.img_size, Image.NEAREST)
            mask = np.array(mask).astype(np.int64)

            image = torch.from_numpy(image).permute(2, 0, 1)
            mask = torch.from_numpy(mask).long()

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        if mask.max() > 11:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: mask.max() = {mask.max()}")
            mask = torch.clamp(mask, 0, 11)

        return image, mask

"""## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ UNet

–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å UNet –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ segmentation_models_pytorch. UNet —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º —ç–Ω–∫–æ–¥–µ—Ä–æ–º ResNet34 –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏.

"""

encoder_name = "resnet34"
encoder_weights = "imagenet"

model = smp.Unet(
    encoder_name=encoder_name,
    encoder_weights=encoder_weights,
    in_channels=3,
    classes=12,
)

print(f"Model: UNet with {encoder_name} encoder")
print(f"Encoder weights: {encoder_weights}")
print(f"Number of classes: 12")
print(f"Number of parameters: {sum(p.numel() for p in model.parameters()):,}")

"""## Lightning –º–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

–°–æ–∑–¥–∞—ë–º PyTorch Lightning –º–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ UNet.

"""

class UNetModule(pl.LightningModule):
    def __init__(self, model, learning_rate=3e-4, use_focal=True):
        super().__init__()
        self.save_hyperparameters(ignore=['model'])

        self.model = model
        self.learning_rate = learning_rate
        self.n_classes = 12
        self.use_focal = use_focal

        # FocalLoss —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –¥–ª—è CamVid
        if use_focal:
            class_weights = torch.tensor([
                0.3, 1.2, 2.5, 0.5, 1.5, 1.0, 3.0, 2.0, 1.0, 4.0, 4.0, 0.0
            ])
            self.loss_fn = FocalLoss(gamma=2.0, alpha=class_weights)
        else:
            self.loss_fn = nn.CrossEntropyLoss(ignore_index=11)

        self.validation_step_outputs = []

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        images, masks = batch
        if masks.dtype != torch.long:
            masks = masks.long()

        logits = self.forward(images)
        loss = self.loss_fn(logits, masks)

        preds = torch.argmax(logits, dim=1)
        iou = self.compute_iou(preds, masks)

        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train_iou', iou, on_step=True, on_epoch=True, prog_bar=True)

        return loss

    def validation_step(self, batch, batch_idx):
        images, masks = batch
        if masks.dtype != torch.long:
            masks = masks.long()

        logits = self.forward(images)
        loss = self.loss_fn(logits, masks)
        preds = torch.argmax(logits, dim=1)

        self.validation_step_outputs.append({'preds': preds, 'masks': masks})
        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)
        return loss

    def on_validation_epoch_end(self):
        if not self.validation_step_outputs:
            self.log('val_iou_epoch', 0.0, prog_bar=True)
            return

        all_preds = torch.cat([x['preds'] for x in self.validation_step_outputs])
        all_masks = torch.cat([x['masks'] for x in self.validation_step_outputs])

        ious = []
        for cls in range(self.n_classes - 1):
            pred_cls = (all_preds == cls)
            target_cls = (all_masks == cls)
            intersection = (pred_cls & target_cls).float().sum()
            union = (pred_cls | target_cls).float().sum()
            iou = (intersection + 1e-6) / (union + 1e-6)
            ious.append(iou)

        mean_iou = torch.stack(ious).mean()
        self.log('val_iou_epoch', mean_iou, prog_bar=True)

        self.validation_step_outputs.clear()

    def compute_iou(self, preds, targets):
        ious = []
        eps = 1e-6
        for cls in range(self.n_classes - 1):
            pred_cls = (preds == cls)
            target_cls = (targets == cls)

            intersection = (pred_cls & target_cls).float().sum((1, 2))
            union = (pred_cls | target_cls).float().sum((1, 2))

            iou = (intersection + eps) / (union + eps)
            mask = union > 0
            if mask.any():
                ious.append(iou[mask].mean())
            else:
                ious.append(torch.tensor(0.0).to(preds.device))

        return torch.stack(ious).mean()

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.learning_rate,
            weight_decay=0.0001
        )

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ total_steps
        total_steps = 1000  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

        if self.trainer and hasattr(self.trainer, 'estimated_stepping_batches'):
            if self.trainer.estimated_stepping_batches is not None:
                total_steps = self.trainer.estimated_stepping_batches
            elif self.trainer.max_epochs > 0:
                # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ
                total_steps = self.trainer.max_epochs * 100

        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=self.learning_rate * 10,
            total_steps=total_steps,
            pct_start=0.3,
            anneal_strategy='cos'
        )

        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'interval': 'step'
            }
        }

"""## –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö

–°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.

"""

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
IMG_SIZE = 256
BATCH_SIZE = 8 if torch.cuda.is_available() else 4
ENCODER = 'resnet34'
LR = 1e-4
USE_FOCAL = True

num_workers = 2 if torch.cuda.is_available() else 0
pin_memory = torch.cuda.is_available()

# –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
data_dir = '/content/CamVid'  # –ò–∑–º–µ–Ω–∏—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
train_transform = get_train_augmentation_256()
val_transform = get_val_augmentation_256()

# –î–∞—Ç–∞—Å–µ—Ç—ã
train_dataset = CamVidDataset(
    data_dir=data_dir,
    split='train',
    img_size=(IMG_SIZE, IMG_SIZE),
    transform=train_transform
)

val_dataset = CamVidDataset(
    data_dir=data_dir,
    split='val',
    img_size=(IMG_SIZE, IMG_SIZE),
    transform=val_transform
)

print(f"Train samples: {len(train_dataset)}")
print(f"Val samples: {len(val_dataset)}")

# –î–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=num_workers,
    pin_memory=pin_memory,
    drop_last=True,
    persistent_workers=False
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=num_workers,
    pin_memory=pin_memory
)

# –ú–æ–¥–µ–ª—å
model = smp.Unet(
    encoder_name=ENCODER,
    encoder_weights='imagenet',
    in_channels=3,
    classes=12,
)
print(f"Model: UNet with {ENCODER} encoder")
print(f"Number of parameters: {sum(p.numel() for p in model.parameters()):,}")

# Lightning –º–æ–¥—É–ª—å
unet_module = UNetModule(
    model=model,
    learning_rate=LR,
    use_focal=USE_FOCAL
)

# Callbacks
checkpoint_callback = ModelCheckpoint(
    monitor='val_iou_epoch',
    mode='max',
    save_top_k=2,
    filename=f'unet-{ENCODER}-256-{{epoch:02d}}-{{val_iou_epoch:.4f}}',
    save_last=True,
    auto_insert_metric_name=False
)

early_stop_callback = EarlyStopping(
    monitor='val_iou_epoch',
    mode='max',
    patience=15,
    min_delta=0.001,
    verbose=True
)

lr_monitor = LearningRateMonitor(logging_interval='step')

# –õ–æ–≥–≥–µ—Ä—ã
logger = [
    TensorBoardLogger('lightning_logs', name=f'unet_{ENCODER}_256'),
    CSVLogger('lightning_logs', name=f'unet_{ENCODER}_256')
]

# Trainer
trainer = pl.Trainer(
    max_epochs=50,
    accelerator='gpu' if torch.cuda.is_available() else 'cpu',
    devices=1,
    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],
    logger=TensorBoardLogger('lightning_logs', name=f'unet_{ENCODER}_{IMG_SIZE}'),
    log_every_n_steps=20,
    val_check_interval=0.25,
    precision='16-mixed' if torch.cuda.is_available() else '32-true',
    gradient_clip_val=0.5,
    accumulate_grad_batches=1,
    enable_progress_bar=True,
    enable_model_summary=False,
)

# –û–±—É—á–µ–Ω–∏–µ
trainer.fit(unet_module, train_loader, val_loader)

"""## –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö

–ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–∞—Å–æ–∫.

"""

fig, axes = plt.subplots(3, 3, figsize=(15, 15))

for i in range(3):
    image, mask = train_dataset[i]

    img_np = image.permute(1, 2, 0).numpy()
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    img_np = img_np * std + mean
    img_np = np.clip(img_np, 0, 1)

    mask_np = mask.numpy()

    axes[i, 0].imshow(img_np)
    axes[i, 0].set_title(f'Image {i+1}')
    axes[i, 0].axis('off')

    axes[i, 1].imshow(mask_np, cmap='tab20', vmin=0, vmax=11)
    axes[i, 1].set_title(f'Mask {i+1}')
    axes[i, 1].axis('off')

    overlay = img_np.copy()
    mask_colored = np.zeros_like(img_np)
    mask_colored[mask_np > 0] = [1, 0, 0]
    overlay = overlay * 0.6 + mask_colored * 0.4

    axes[i, 2].imshow(overlay)
    axes[i, 2].set_title(f'Overlay {i+1}')
    axes[i, 2].axis('off')

plt.tight_layout()
plt.show()

print(f"Image shape: {image.shape}")
print(f"Mask shape: {mask.shape}")
print(f"Mask unique values: {torch.unique(mask)}")

"""## –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏

–°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å UNet –∏ Lightning –º–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.

"""

unet_module = UNetModule(
    model=model,
    learning_rate=1e-4
)

sample_input = torch.randn(1, 3, 256, 256)
with torch.no_grad():
    sample_output = unet_module(sample_input)

print(f"Input shape: {sample_input.shape}")
print(f"Output shape: {sample_output.shape}")
print(f"Model parameters: {sum(p.numel() for p in unet_module.parameters()):,}")

"""## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º callbacks –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyTorch Lightning.

"""

checkpoint_callback = ModelCheckpoint(
    monitor='val_iou_epoch',                # –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ
    mode='max',
    save_top_k=1,
    filename='unet-{epoch:02d}-{val_iou_epoch:.4f}',  # –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ
    save_last=True
)

early_stop_callback = EarlyStopping(
    monitor='val_iou_epoch',                # –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ
    mode='max',
    patience=10,
    verbose=True
)

logger = TensorBoardLogger('lightning_logs', name='unet_segmentation')

trainer = pl.Trainer(
    max_epochs=50,
    accelerator='gpu' if torch.cuda.is_available() else 'cpu',
    devices=1,
    callbacks=[checkpoint_callback, early_stop_callback],
    logger=logger,
    log_every_n_steps=10,
    val_check_interval=0.5
)

trainer.fit(unet_module, train_loader, val_loader)

"""## –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ

–û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.

"""

best_model_path = checkpoint_callback.best_model_path
print(f"Best model path: {best_model_path}")

"""## –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞.

"""

unet_module.eval()
device = next(unet_module.parameters()).device

num_samples = 6
fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4 * num_samples))

with torch.no_grad():
    for idx in range(num_samples):
        image, mask = val_dataset[idx]
        image_batch = image.unsqueeze(0).to(device)

        logits = unet_module(image_batch)
        pred = torch.argmax(logits, dim=1).cpu().squeeze()

        img_np = image.permute(1, 2, 0).numpy()
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        img_np = img_np * std + mean
        img_np = np.clip(img_np, 0, 1)

        mask_np = mask.numpy()
        pred_np = pred.numpy()

        axes[idx, 0].imshow(img_np)
        axes[idx, 0].set_title('Original Image')
        axes[idx, 0].axis('off')

        axes[idx, 1].imshow(mask_np, cmap='tab20', vmin=0, vmax=11)
        axes[idx, 1].set_title('Ground Truth')
        axes[idx, 1].axis('off')

        axes[idx, 2].imshow(pred_np, cmap='tab20', vmin=0, vmax=11)
        axes[idx, 2].set_title('Prediction')
        axes[idx, 2].axis('off')

        overlay = img_np.copy()
        pred_colored = np.zeros_like(img_np)
        pred_colored[pred_np > 0] = [1, 0, 0]
        overlay = overlay * 0.6 + pred_colored * 0.4

        axes[idx, 3].imshow(overlay)
        axes[idx, 3].set_title('Prediction Overlay')
        axes[idx, 3].axis('off')

        correct = (pred_np == mask_np).sum()
        total = mask_np.size
        pixel_acc = correct / total
        axes[idx, 3].text(10, 20, f'Acc: {pixel_acc:.3f}',
                          bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
                          fontsize=12, color='black')

plt.tight_layout()
plt.show()

"""## –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ

–í—ã—á–∏—Å–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.

"""

def compute_metrics(preds, targets, n_classes=12):
    ious = []
    for cls in range(n_classes):
        pred_cls = (preds == cls)
        target_cls = (targets == cls)
        intersection = (pred_cls & target_cls).float().sum((1, 2))
        union = (pred_cls | target_cls).float().sum((1, 2))
        iou = (intersection + 1e-6) / (union + 1e-6)
        ious.append(iou.mean().item())

    mean_iou = np.mean(ious)

    correct = (preds == targets).float().sum((1, 2))
    total = targets.numel() // targets.shape[0]
    pixel_acc = correct.sum().item() / (targets.shape[0] * total)

    return {
        'mean_iou': mean_iou,
        'iou_per_class': ious,
        'pixel_acc': pixel_acc
    }

unet_module.eval()
all_preds = []
all_targets = []

val_loader_eval = DataLoader(
    val_dataset,
    batch_size=8 if torch.cuda.is_available() else 4,
    shuffle=False,
    num_workers=num_workers,
    pin_memory=True
)

with torch.no_grad():
    for images, masks in tqdm(val_loader_eval, desc="Evaluating"):
        images = images.to(device)
        logits = unet_module(images)
        preds = torch.argmax(logits, dim=1).cpu()

        all_preds.append(preds)
        all_targets.append(masks)

all_preds = torch.cat(all_preds, dim=0)
all_targets = torch.cat(all_targets, dim=0)

metrics = compute_metrics(all_preds, all_targets, n_classes=12)

camvid_classes = ['Sky', 'Building', 'Column-Pole', 'Road', 'Sidewalk', 'Tree', 'Sign-Symbol', 'Fence', 'Car', 'Pedestrian', 'Bicyclist', 'Void']

print("="*60)
print("Validation Metrics:")
print("="*60)
print(f"Mean IoU: {metrics['mean_iou']:.4f}")
print(f"Pixel Accuracy: {metrics['pixel_acc']:.4f}")
print("\nIoU per class:")
for cls_name, iou in zip(camvid_classes, metrics['iou_per_class']):
    print(f"  {cls_name}: {iou:.4f}")
print("="*60)